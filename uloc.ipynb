{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install MAMBAPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mambapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dataloaders.params import INPUT_NAMES\n",
    "from dataloaders.mcd_uwbloader import MCDLoader\n",
    "from dataloaders.load_data import load_data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import open3d as o3d\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# from PIL import Image\n",
    "# from IPython import display\n",
    "import seaborn as sns   # For violin plot\n",
    "\n",
    "from util.eval import Result\n",
    "from util.trainer import train,validate,network_train\n",
    "from util.visualization import visualize_sample\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from dataloaders.mcd_uwbloader import UwbDataLoader,obtain_scale\n",
    "from models import mambaNet, rnn_attention\n",
    "from models.load_model import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train         = True        # Set to False to skip training, automatically load the pretrained model\n",
    "save_weights     = True         # whether save weights\n",
    "training_dataset = 'MCD'        # selected from MCD and RONET\n",
    "dataset_path     = './data'     # the path for the data dir\n",
    "training_model   = 'MAMBA'      # selected from [MAMBA,BiLSTM,LSTM,GRU,RNN,BiRNN]\n",
    "train_one_tag    = False        # only valid train on MCD dataset\n",
    "train_on_slam    = False        # only valid train on MCD dataset\n",
    "min_anc          = 1            # filter data at least min_anc number of anchors data is available\n",
    "batchsize        = 64\n",
    "seqlen           = 20           # the time length of the input uwb data L\n",
    "coding_dim       = 128          # the dim of uwb embedding\n",
    "hiddendim        = 256          # hidden size dimention of RNN based model\n",
    "num_layers       = 4            # the num of layers for MAMBA\n",
    "lr               = 0.001        # learning rate\n",
    "weigt_decay      = 0.9\n",
    "decay_rate       = 0.5\n",
    "decay_step       = 20\n",
    "num_epoch        = 100\n",
    "device           = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if training_dataset==\"MCD\":\n",
    "    if train_one_tag:\n",
    "        input_dim    = 10\n",
    "        output_dim   = 3\n",
    "    else:\n",
    "        input_dim    = 20\n",
    "        output_dim   = 6\n",
    "else:\n",
    "    input_dim    = 8\n",
    "    output_dim   = 2\n",
    "    train_one_tag = True\n",
    "\n",
    "\n",
    "save_dir = f\"output/{training_dataset}/checkpoints/{training_model}\"\n",
    "log_dir  = f\"output/{training_dataset}/logs/{training_model}\"\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "os.makedirs(log_dir,exist_ok=True)\n",
    "save_path = os.path.join(save_dir,\"best_model\")\n",
    "saved_checkpoint_path = save_path  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train on {training_dataset} dataset\")\n",
    "\n",
    "if training_dataset==\"MCD\":\n",
    "    train_dataset ,val_dataset,train_dataloader,val_dataloader,Xscaler, Yscaler = load_data(seqlen,batchsize,min_anc,training_dataset,train_on_slam,train_one_tag,dataset_path)\n",
    "    test_dataloader = val_dataloader\n",
    "else:\n",
    "    train_dataloader,val_dataloader,test_dataloader, Yscaler = load_data(seqlen,batchsize,min_anc,training_dataset,train_on_slam,train_one_tag,dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(training_model, input_dim,output_dim,coding_dim,num_layers, hiddendim, device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "if is_train:\n",
    "    print(f\"Train on {training_dataset} dataset, using model {training_model}\")\n",
    "    vis = 1 # 1: plot the loss curve, 0: print the loss value\n",
    "    res = network_train(model,training_dataset,train_dataloader, val_dataloader, Yscaler, criterion,optimizer,num_epoch,device,lr,decay_rate,decay_step,vis=vis,save_weights=save_weights,save_path=save_path,log_path=log_dir,half=train_one_tag,real=train_on_slam,)\n",
    "else:\n",
    "    print(f\"Test on {training_dataset} dataset, using model {training_model}\")\n",
    "    model = torch.load(saved_checkpoint_path).to(device)\n",
    "    res = validate(test_dataloader, model, training_dataset, 1, 'Test',device,Yscaler,half=train_one_tag)\n",
    "    print((res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test on {training_dataset} dataset, the using model is {training_model}, load weights from: {saved_checkpoint_path}\")\n",
    "model = torch.load(saved_checkpoint_path).to(device)\n",
    "model.eval()\n",
    "res = validate(test_dataloader, model, training_dataset, 1, 'Test', device, Yscaler,half=train_one_tag)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the test results on MCD NTU dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.visualization import plotPathError\n",
    "# Plot the error based on location\n",
    "test_dataset = val_dataset\n",
    "# Initialize variables for chunk processing\n",
    "chunk_size = 1000\n",
    "total_data = test_dataset.getX().shape[0]\n",
    "num_chunks = total_data // chunk_size\n",
    "testErrScaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Initialize lists to accumulate results\n",
    "all_Y = []\n",
    "all_Ypred = []\n",
    "all_errors = []\n",
    "\n",
    "# Iterate over each chunk\n",
    "for i in range(num_chunks):\n",
    "    # Load a chunk of data\n",
    "    X = test_dataset.getX()[i*chunk_size:(i+1)*chunk_size, :, :]\n",
    "    Y = test_dataset.getY()[i*chunk_size:(i+1)*chunk_size, :, :]\n",
    "    Y = torch.cat((Y[0, :, :], Y[:, -1, :]), dim=0)\n",
    "    Y = Yscaler.inverse_transform(Y.cpu())\n",
    "\n",
    "    Ypred = model(X.float().to(device)).cpu().detach()\n",
    "    Ypred = torch.cat((Ypred[0, :, :], Ypred[:, -1, :]), dim=0)\n",
    "    Ypred = Yscaler.inverse_transform(Ypred.cpu())\n",
    "\n",
    "    test_res = Result(half=train_one_tag)\n",
    "    test_res.evaluate(Ypred, Y)\n",
    "    test_err = np.linalg.norm(test_res.abs_diff, axis=1)\n",
    "\n",
    "    # Accumulate results\n",
    "    all_Y.append(Y)\n",
    "    all_Ypred.append(Ypred)\n",
    "    all_errors.append(test_err)\n",
    "\n",
    "    # Clear memory for this chunk\n",
    "    del X, Y, Ypred, test_res, test_err\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Combine accumulated results into a single array\n",
    "all_Y = np.vstack(all_Y)\n",
    "all_Ypred = np.vstack(all_Ypred)\n",
    "all_errors = np.concatenate(all_errors)\n",
    "\n",
    "# Calculate global statistics for error clamping\n",
    "mean = np.mean(all_errors)\n",
    "colorScalePiv = mean + 2 * np.var(all_errors)\n",
    "\n",
    "# Clamp the error color globally\n",
    "all_errors[all_errors > colorScalePiv] = colorScalePiv\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "all_Y, all_Ypred = plotPathError(all_Y, all_Ypred, ax,colorScalePiv, train_on_slam, train_one_tag, trial='All')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the error for each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_on_slam:\n",
    "    test_path = os.path.join(dataset_path,\"MCDUWB_slamprior/test\")\n",
    "else:\n",
    "    test_path = os.path.join(dataset_path,\"MCDUWB_gndtruth/test\")\n",
    "\n",
    "chunk_size = 1000  # Size of each chunk\n",
    "test_seq = glob.glob(test_path + '/*.csv')\n",
    "figsize = (8, 12)\n",
    "test_df = None\n",
    "\n",
    "for seqidx, seq in enumerate(test_seq):\n",
    "    \n",
    "    seq_name = os.path.basename(seq).replace('_packed.csv', '')\n",
    "    # Load dataset and dataloader\n",
    "    seq_dataset = MCDLoader(seq, seqlen=seqlen, Xscaler=Xscaler, Yscaler=Yscaler,min_anc=min_anc,half=train_one_tag)\n",
    "    X = seq_dataset.getX()\n",
    "    Y = seq_dataset.getY()\n",
    "    total_data = X.shape[0]\n",
    "    num_chunks = total_data // chunk_size\n",
    "\n",
    "    all_Y = []\n",
    "    all_Ypred = []\n",
    "    all_errors = []\n",
    "\n",
    "    # Process each chunk\n",
    "    for i in range(num_chunks):\n",
    "        # print(f\"Processing chunk {i + 1}/{num_chunks} of sequence {seqidx + 1}/{len(test_seq)}...\")\n",
    "\n",
    "        # Load a chunk of data\n",
    "        X_chunk = X[i*chunk_size:(i+1)*chunk_size, :, :]\n",
    "        Y_chunk = Y[i*chunk_size:(i+1)*chunk_size, :, :]\n",
    "        Y_chunk = torch.cat((Y_chunk[0, :, :], Y_chunk[:, -1, :]), dim=0)\n",
    "        Y_chunk = Yscaler.inverse_transform(Y_chunk.cpu())\n",
    "\n",
    "        # Predict\n",
    "        Ypred_chunk = model(X_chunk.float().to(device)).cpu().detach()\n",
    "        Ypred_chunk = torch.cat((Ypred_chunk[0, :, :], Ypred_chunk[:, -1, :]), dim=0)\n",
    "        Ypred_chunk = Yscaler.inverse_transform(Ypred_chunk.cpu())\n",
    "\n",
    "        # Evaluate errors\n",
    "        chunk_res = Result(half=train_one_tag)\n",
    "        chunk_res.evaluate(Ypred_chunk, Y_chunk)\n",
    "        chunk_err = np.linalg.norm(chunk_res.abs_diff, axis=1)\n",
    "\n",
    "        # Accumulate results\n",
    "        all_Y.append(Y_chunk)\n",
    "        all_Ypred.append(Ypred_chunk)\n",
    "        all_errors.append(chunk_err)\n",
    "\n",
    "        # Clear memory for this chunk\n",
    "        del X_chunk, Y_chunk, Ypred_chunk, chunk_res, chunk_err\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Combine results of all chunks\n",
    "    all_Y = np.vstack(all_Y)\n",
    "    all_Ypred = np.vstack(all_Ypred)\n",
    "    all_errors = np.concatenate(all_errors)\n",
    "    \n",
    "    # Plot the results for the current sequence\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=figsize)\n",
    "\n",
    "    all_Y1, all_Ypred1 = plotPathError(all_Y, all_Ypred, ax[0],colorScalePiv,train_on_slam,train_one_tag)\n",
    "\n",
    "    # Calculate RMSE for the entire sequence\n",
    "    seq_res = Result(half=train_one_tag)\n",
    "    seq_res.evaluate(all_Ypred, all_Y)\n",
    "\n",
    "\n",
    "    # Real metric Error\n",
    "    err = np.linalg.norm(seq_res.abs_diff, axis=1)\n",
    "\n",
    "    df = pd.DataFrame(err, columns=['error'])\n",
    "    df['seq'] = seq_name\n",
    "\n",
    "    # Concatenate the DataFrame for later representation\n",
    "    test_df = df if test_df is None else pd.concat([test_df, df])\n",
    "\n",
    "    all_Ypred2, all_Y2 = plotPathError(all_Ypred, all_Y, ax[1],colorScalePiv, train_on_slam, train_one_tag, trial=seq_name)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "\n",
    "# Map sequence to camp (here it's 'mcd')\n",
    "test_df['camp'] = test_df['seq'].apply(lambda x: 'mcd')\n",
    "\n",
    "# Get the list of sequences and sort them\n",
    "sequences = sorted(list(set(test_df['seq'])))\n",
    "Nseq = len(sequences)\n",
    "\n",
    "# Generate colors for each sequence\n",
    "colors = []\n",
    "for i in range(Nseq):\n",
    "    rgb = colorsys.hsv_to_rgb(i / Nseq, 0.5, 1.0)\n",
    "    colors.append(rgb)\n",
    "\n",
    "figsize = (8, 5)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figsize)\n",
    "dx = 'seq'\n",
    "dy = 'error'\n",
    "sigma = 0.6\n",
    "ort = \"v\"\n",
    "cut = 0.0\n",
    "\n",
    "# Plot the violin plots\n",
    "for idx, seq in enumerate(sequences):\n",
    "    df = test_df[test_df['seq'] == seq].copy()\n",
    "    df['xtick'] = df['seq'].apply(lambda x: -1)  # xtick position for violin plot\n",
    "\n",
    "    # Draw violin plot\n",
    "    violin = sns.violinplot(\n",
    "        ax=ax, x='xtick', y=dy, data=df, scale=\"width\", inner=None, color=colors[idx]\n",
    "    )\n",
    "\n",
    "    # Modify to only draw half of the violin\n",
    "    v = violin.collections[-1]\n",
    "    m = np.mean(v.get_paths()[0].vertices[:, 0])\n",
    "    v.get_paths()[0].vertices[:, 0] = np.clip(v.get_paths()[0].vertices[:, 0], -np.inf, m)\n",
    "    v.set_edgecolor('none')\n",
    "\n",
    "    plt.xticks([-1], [''])\n",
    "\n",
    "# Plot the box plots\n",
    "for idx, seq in enumerate(sequences):\n",
    "    df = test_df[test_df['seq'] == seq].copy()\n",
    "    df['xtick'] = idx  # xtick position for box plot\n",
    "\n",
    "    ax.boxplot(\n",
    "        df['error'],\n",
    "        positions=[0.25 + idx * 0.25],\n",
    "        patch_artist=True,\n",
    "        medianprops=dict(color='black'),\n",
    "        boxprops=dict(facecolor=colors[idx], edgecolor='none'),\n",
    "        showfliers=False\n",
    "    )\n",
    "    plt.xticks([-1], [''])\n",
    "\n",
    "# Add legend\n",
    "handles = [plt.Line2D([], [], color=color, marker='s', linestyle='None') for color in colors]\n",
    "labels = [seq.replace('mcd_', '') for seq in sequences]  # Labels for each sequence\n",
    "\n",
    "# ax.legend(handles, labels)\n",
    "\n",
    "# Format the plot\n",
    "ax.set_xlim([-0.5, 1.5])\n",
    "ax.set_ylim([0, 10])\n",
    "\n",
    "ax.set_xlabel('mcd test sequences')\n",
    "ax.set_ylabel('Inference Error [m]')\n",
    "\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3810",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
